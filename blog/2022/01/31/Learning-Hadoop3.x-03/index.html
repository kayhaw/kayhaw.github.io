<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.6">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Kay Haw&#39;s Blog Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Kay Haw&#39;s Blog Blog Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">Hadoop 3.x学习笔记(3) | Kay Haw&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://kayhaw.github.io/blog/2022/01/31/Learning-Hadoop3.x-03"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="Hadoop 3.x学习笔记(3) | Kay Haw&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Hadoop 3.1.3学习笔记(3)"><meta data-react-helmet="true" property="og:description" content="Hadoop 3.1.3学习笔记(3)"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2022-01-31T00:00:00.000Z"><meta data-react-helmet="true" property="article:author" content="https://github.com/kayhaw"><meta data-react-helmet="true" property="article:tag" content="Hadoop,BigData"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://kayhaw.github.io/blog/2022/01/31/Learning-Hadoop3.x-03"><link data-react-helmet="true" rel="alternate" href="https://kayhaw.github.io/blog/2022/01/31/Learning-Hadoop3.x-03" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://kayhaw.github.io/blog/2022/01/31/Learning-Hadoop3.x-03" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.0138175c.css">
<link rel="preload" href="/assets/js/runtime~main.ed6f7924.js" as="script">
<link rel="preload" href="/assets/js/main.87f14a62.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><img src="/img/logo.svg" alt="Kay Haw&#x27;s Blog" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/img/logo.svg" alt="Kay Haw&#x27;s Blog" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><b class="navbar__title">Kay Haw&#x27;s Blog</b></a><a class="navbar__item navbar__link" href="/docs/intro">读书笔记</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/kayhaw" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="react-toggle toggle_3Zt9 react-toggle--disabled"><div class="react-toggle-track" role="button" tabindex="-1"><div class="react-toggle-track-check"><span class="toggle_71bT">🌜</span></div><div class="react-toggle-track-x"><span class="toggle_71bT">🌞</span></div><div class="react-toggle-thumb"></div></div><input type="checkbox" class="react-toggle-screenreader-only" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_2ahu thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_2hhb margin-bottom--md">Recent posts</div><ul class="sidebarItemList_2xAf"><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/2022/02/10/Learning-Hadoop3.x-06">Hadoop 3.x学习笔记(6)</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/2022/02/07/Learning-Hadoop3.x-04">Hadoop 3.x学习笔记(4)</a></li><li class="sidebarItem_2UVv"><a aria-current="page" class="sidebarItemLink_1RT6 sidebarItemLinkActive_12pM" href="/blog/2022/01/31/Learning-Hadoop3.x-03">Hadoop 3.x学习笔记(3)</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/2022/01/30/Learning-Hadoop3.x-02">Hadoop 3.x学习笔记(2)</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/2022/01/29/Learning-Hadoop3.x-01">Hadoop 3.x学习笔记(1)</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_GeHD" itemprop="headline">Hadoop 3.x学习笔记(3)</h1><div class="blogPostData_291c margin-vert--md"><time datetime="2022-01-31T00:00:00.000Z" itemprop="datePublished">January 31, 2022</time> · 5 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><a href="https://github.com/kayhaw" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_1yU8" src="https://avatars.githubusercontent.com/u/16892835?v=4" alt="何轲"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/kayhaw" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">何轲</span></a></div><small class="avatar__subtitle" itemprop="description">Never settle down</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>📝Hadoop 3.1.3学习笔记第3篇：MapReduce。</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="mapreduce简介"></a>MapReduce简介<a class="hash-link" href="#mapreduce简介" title="Direct link to heading">#</a></h2><p>MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带组件整合成完整的分布式运算程序，然后在Hadoop集群上并发运行。</p><p>优点😄：</p><ol><li>易于编程，用户只需要关系业务逻辑；</li><li>扩展性好，可以动态增加服务器；</li><li>高容错性，计算节点挂点可以将任务转移到其他节点；</li><li>适合海量数据计算(TB/PB)。</li></ol><p>缺点😠：</p><ol><li>不适合实时计算；</li><li>不适合流式计算；</li><li>不适合DAG有向无环图计算。</li></ol><p>MapReduce程序分为2个阶段：Map阶段和Reduce阶段。Map阶段的MapTask并发运行互不相干，Reduce阶段的ReduceTask也互不相关，但依赖于MapTask的输出。MapReduce程序只能包含一个Map阶段和一个Reduce阶段，如果业务逻辑非常复杂，那么只能多个MapReduce程序串行执行。</p><p>MR程序编写分为3部分：Mapper、Reducer和Driver。</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="mapper类"></a>Mapper类<a class="hash-link" href="#mapper类" title="Direct link to heading">#</a></h3><ol><li>自定义类继承<code>org.apache.hadoop.mapreduce.Mapper</code>类；</li><li>定义输入K-V泛型，即泛型参数列表的第1、2个参数；</li><li>定义输出K-V泛型，即泛型列表的第3、4个参数；</li><li>map方法中实现业务逻辑；</li><li><strong>map()方法（MapTask进程）对每一个K-V对调用一次</strong>。</li></ol><div class="codeBlockContainer_K1bP"><div style="color:#393A34;background-color:#f6f8fa" class="codeBlockTitle_eoMF">WordCountMapper.java</div><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    // 减少对象创建次数，不要放到map里面</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    private Text outK = new Text();</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    private IntWritable outV = new IntWritable(1);</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    @Override</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 1. 获取一行</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        String line = value.toString();</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 2. 切分</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        String[] words = line.split(&quot; &quot;);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 3. 循环写出</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        for (String word : words) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">            outK.set(word);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">            context.write(outK, outV);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="reducer类"></a>Reducer类<a class="hash-link" href="#reducer类" title="Direct link to heading">#</a></h3><ol><li>自定义类继承<code>org.apache.hadoop.mapreduce.Reducer</code>类；</li><li>输入K-V泛型是对应Mapper的输出K-V泛型；</li><li>reduce方法实现业务逻辑；</li><li><strong>ReduceTask进程对每一组相同key的K-V组调用一次reduce()方法</strong>。</li></ol><div class="codeBlockContainer_K1bP"><div style="color:#393A34;background-color:#f6f8fa" class="codeBlockTitle_eoMF">WordCountReducer.java</div><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    private IntWritable outV = new IntWritable();</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    @Override</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException,</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        InterruptedException {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        int sum = 0;</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        for (IntWritable value : values) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">            sum += value.get();</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        outV.set(sum);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        context.write(key, outV);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="driver类"></a>Driver类<a class="hash-link" href="#driver类" title="Direct link to heading">#</a></h3><p>相当于YARN集群的客户端，用于提交MR任务到YARN集群。</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">public class WordCountDriver {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 1. 获取Job</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        Configuration conf = new Configuration();</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        Job job = Job.getInstance(conf);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 2. 设置jar路径</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        job.setJarByClass(WordCountDriver.class);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 3. 关联mapper和reducer</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        job.setMapperClass(WordCountMapper.class);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        job.setReducerClass(WordCountReducer.class);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 4. 设置mapper输出KV类型</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        job.setMapOutputKeyClass(Text.class);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        job.setMapOutputValueClass(IntWritable.class);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 5. 设置最终输出的KV类型</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        job.setOutputKeyClass(Text.class);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        job.setOutputValueClass(IntWritable.class);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 6. 设置输入输出路径</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        FileInputFormat.setInputPaths(job, new Path(&quot;wcinput\\words.txt&quot;));</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        FileOutputFormat.setOutputPath(job, new Path(&quot;wcoutput&quot;));</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        // 7. 提交Job</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        boolean result = job.waitForCompletion(true);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        System.exit(result ? 0 : 1);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>注意</h5></div><div class="admonition-content"><ol><li>注意导入org.apache.hadoop.mapreduce.*而不是org.apache.hadoop.mapred.*，前者是Hadoop 2.x/3.x使用，后者是Hadoop 1.x；</li><li>WordCountDriver中设置输出KV类型代码写错，导致类型不一致报错。</li></ol></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="序列化"></a>序列化<a class="hash-link" href="#序列化" title="Direct link to heading">#</a></h2><p>序列化是将内存中对象转为字节序列以便于持久化和网络传输的过程，反序列化是将字节序列转为内存中对象的过程。Java序列化机制(Serializable)过于重量，附带校验信息、Header、继承体系等，不便于网络中高效传输，因此Hadoop自定义了序列化机制(Writable)，具有以下特性：</p><ol><li>紧凑：高效使用存储空间；</li><li>快速：读写数据开销小；</li><li>互操作：支持多语言交互。</li></ol><p>对于Java中的基本类型，Hadoop提供如下对应序列化类：</p><table><thead><tr><th>Java类型</th><th>Hadoop Writable类型</th></tr></thead><tbody><tr><td>Boolean</td><td>BooleanWritable</td></tr><tr><td>Byte</td><td>ByteWritable</td></tr><tr><td>Int</td><td>IntWritable</td></tr><tr><td>Float</td><td>FloatWritable</td></tr><tr><td>Long</td><td>LongWritable</td></tr><tr><td>Double</td><td>DoubleWritable</td></tr><tr><td>String</td><td>Text</td></tr><tr><td>Map</td><td>MapWritable</td></tr><tr><td>Array</td><td>ArrayWritable</td></tr><tr><td>Null</td><td>NullWritable</td></tr></tbody></table><p>构建符合Hadoop序列化机制的Bean类需要满足如下条件：</p><ol><li>实现Writable接口；</li><li>提供一个空参构造器(序列化反射时用)；</li><li>重写write和readFields方法，<strong>注意字段读写顺序一致</strong>；</li><li><strong>当Bean类作为Key使用时还需要实现Comparable接口</strong>(Shuffle过程使用)；</li><li>当需要序列化到文件时需要重写toString方法。</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="mapreduce框架"></a>MapReduce框架<a class="hash-link" href="#mapreduce框架" title="Direct link to heading">#</a></h2><p>如下图所示，MR框架包含4个组件：InputFormat、Mapper、Reducer和OutputFormat。MapReduce程序执行流程为：由InputFormat将输入数据分为切片(Split)，Mapper调用map方法处理每个切片中数据，通过Shuffle将处理后结果交给Reducer，它又调用reduce方法合并切片结果，最后通过OutputFormat将合并结果输出。</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="job提交流程"></a>Job提交流程<a class="hash-link" href="#job提交流程" title="Direct link to heading">#</a></h3><p>在WordCountDriver中调用waitForCompletion方法来提交作业并获取结果，该方法调用流程如下：</p><ol><li><p>调用submit方法，该方法流程如下：</p><ol><li>调用ensureState方法判断当前提交状态，必须是JobState.DEFINE；</li><li>调用setUseNewAPI方法处理新老mapreduce API的兼容性；</li><li>调用connect方法创建Cluster对象，该类的构造方法调用initialize方法，该方法又依次遍历YarnClientProtocolProvider和LocalClientProtocolProvider，确定客户端为LocalJobRunner；</li><li>调用submitJobInternal方法。</li></ol></li><li><p>展开分析submitJobInternal方法流程：</p><ol><li>调用checkSpecs方法，该方法调用OutputFormat的checkOutputSpecs方法检查输出配置(<strong>对于FileOutputFormat来说它检查输出路径不能为空并且不能已经存在</strong>)；</li><li>调用JobSubmissionFiles.getStagingDir()方法生成一个stage文件夹；</li><li>调用ClientProtocol.getNewJobID()生成jobId；</li><li>stage路径和jobId组成路径submitJobDir(例如/tmp/hadoop/mapred/staging/xiaok1378532364/.staging/job_local1378532364_0001)；</li><li>通过调用链copyAndConfigureFiles-&gt;uploadResources-&gt;uploadResourcesInternal方法，进行生成submitJobDir文件夹，将程序jar包上传到集群(uploadJobJar)等操作；</li><li>调用InputFormat的writeSplits方法得到切片数maps，在submitJobDir下生成切片信息文件(<code>.job.split.crc</code>、<code>.job.splitmetainfo.crc</code>、<code>job.split</code>、<code>job.splitmetainfo</code>)，随后将切片数设置到Configuration中；</li><li>调用writeConf方法，在submitJobDir下生成<code>job.xml</code>(包含本次作业执行所有的配置信息)和<code>.job.xml.crc</code>；</li><li>调用ClientProtocol.submitJob方法提交作业；</li></ol></li><li><p>将state设置为JobState.RUNNING，回到waitForCompletion；</p></li><li><p>最后返回isSuccessful()方法结果。</p></li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="fileinputformat切片"></a>FileInputFormat切片<a class="hash-link" href="#fileinputformat切片" title="Direct link to heading">#</a></h3><p>writeSplits方法根据是否使用新MapReduce API选择调用writeNewSplits方法或者writeOldSplits，这里分析writeNewSplits代码，它调用InputFormat的getSplits抽象方法。抽象类InputFormat的各个子类xxxInputFormat会实现getSplits方法，以FileInputFormat的getSplits方法为例分析如下：</p><ol><li>获得minSize=1，maxSize=Long.MAX_VALUE；</li><li>循环遍历输入路径下的所有文件执行步骤3-6；</li><li>由isSplitable方法判断该文件是否可切片，FileInputFormat默认为true；</li><li>获取块大小blockSize，默认是33554432(本地调试为32MB)；</li><li>计算切片大小splitSize=Math.max(minSize, Math.min(maxSize, blockSize))，因此默认切片大小就是块大小；</li><li>while循环生成切片信息InputSplit列表，注意只有剩余长度/splitSize大于SPLIT_SLOP(默认0.1)时才会切片。</li></ol><p>接着调用JobSplitWriter.createSplitFiles在subJobDir中生成切片规划文件，将其提交到YARN上后，MrAppMaster根据切片规划文件来计算MapTask个数。</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="textinputformat和combinefileinputformat"></a>TextInputFormat和CombineFileInputFormat<a class="hash-link" href="#textinputformat和combinefileinputformat" title="Direct link to heading">#</a></h3><p>FileInputFormat抽象类下针对不同使用场景派生了NLineInputFormat、TextInputFormat、CombineFileInputFormat、KeyValueTextInputFormat等子类，本节介绍常用的TextInputFormat和CombineFileInputFormat。</p><p>TextInputFormat是FileInputFormat的默认实现类，源码如下：</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">public class TextInputFormat extends FileInputFormat&lt;LongWritable, Text&gt; {</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  @Override</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  public RecordReader&lt;LongWritable, Text&gt; </span></span><span class="token-line" style="color:#393A34"><span class="token plain">    createRecordReader(InputSplit split,</span></span><span class="token-line" style="color:#393A34"><span class="token plain">                       TaskAttemptContext context) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    String delimiter = context.getConfiguration().get(</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;textinputformat.record.delimiter&quot;);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    byte[] recordDelimiterBytes = null;</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    if (null != delimiter)</span></span><span class="token-line" style="color:#393A34"><span class="token plain">      recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    return new LineRecordReader(recordDelimiterBytes);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  @Override</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  protected boolean isSplitable(JobContext context, Path file) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    final CompressionCodec codec =</span></span><span class="token-line" style="color:#393A34"><span class="token plain">      new CompressionCodecFactory(context.getConfiguration()).getCodec(file);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    if (null == codec) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">      return true;</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    return codec instanceof SplittableCompressionCodec;</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>TextInputFormat切片按照文件一个一个来，不管文件有多小都至少是一个切片，对于大量小文件而言这回产生大量MapTask降低处理效率。而CombineFileInputFormat用于处理这种大量小文件的情况，通过CombineTextInputFormat.setMaxInputSplitSize设置虚拟存储切片大小maxInputSplitSize，并按照如下过程切片：</p><ol><li>循环遍历输入文件，得到文件大小fileSize；</li><li>fileSize &lt; maxInputSplitSize，逻辑上划分为一块虚拟存储；</li><li>2 * maxInputSplitSize &lt; fileSize，以maxInputSplitSize切分一块；</li><li>maxInputSplitSize &lt; fileSize &lt;  2 * maxInputSplitSize，平均分成2块；</li><li>遍历所有虚拟存储块，判断大小virtBlockSize是否大于maxInputSplitSize</li><li>virtBlockSize大于等于maxInputSplitSize，形成一个切片；</li><li>virtBlockSize小于maxInputSplitSize，与下一个虚拟存储块合并形成一个切片。</li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="shuffle"></a>Shuffle<a class="hash-link" href="#shuffle" title="Direct link to heading">#</a></h3><p>map方法之后reduce方法之前的数据处理过程，包含分区、排序、Combiner等操作。</p><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="分区策略"></a>分区策略<a class="hash-link" href="#分区策略" title="Direct link to heading">#</a></h4><p>如下代码所示，当未通过<code>Job.setNumReduceTasks(int tasks)</code>设置任务数量时，默认partitions为1，此时getPartition返回0，即只有一个分区。</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span></span><span class="token-line" style="color:#393A34"><span class="token plain">                       JobConf job,</span></span><span class="token-line" style="color:#393A34"><span class="token plain">                       TaskUmbilicalProtocol umbilical,</span></span><span class="token-line" style="color:#393A34"><span class="token plain">                       TaskReporter reporter</span></span><span class="token-line" style="color:#393A34"><span class="token plain">                       ) throws IOException, ClassNotFoundException {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  collector = createSortingCollector(job, reporter);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  partitions = jobContext.getNumReduceTasks();</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  if (partitions &gt; 1) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span></span><span class="token-line" style="color:#393A34"><span class="token plain">      ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  } else {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    partitioner = new org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">      @Override</span></span><span class="token-line" style="color:#393A34"><span class="token plain">      public int getPartition(K key, V value, int numPartitions) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        return partitions - 1;</span></span><span class="token-line" style="color:#393A34"><span class="token plain">      }</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    };</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>若设置数量大于1，则通过getPartitionerClass()方法获取分区类并通过反射实例化，默认使用HashPartitioner，也可以通过参数<code>mapreduce.job.partitioner.class</code>指定。此时获取分区编号通过key的hash值对任务数量取模得到，如下所示：</p><div class="codeBlockContainer_K1bP"><div style="color:#393A34;background-color:#f6f8fa" class="codeBlockTitle_eoMF">HashPartitioner.java</div><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; {</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  /** Use {@link Object#hashCode()} to partition. */</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  public int getPartition(K key, V value,</span></span><span class="token-line" style="color:#393A34"><span class="token plain">                          int numReduceTasks) {</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>因此，自定义分区策略有如下3步：</p><ol><li>自定义CustomPartitioner类继承Partitioner抽象类，实现getPartition抽象方法；</li><li>在驱动类中注册使用，<code>job.setPartitionerClass(CustomPartitioner.class)</code>；</li><li>设置reduce任务数量，<code>job.setNumReduceTasks(2)</code>。</li></ol><p>注意getPartition方法返回的分区号必须从0开始，逐一增加。如果设置reduce任务数量和getPartition返回值不同，根据两者关系有如下情况：</p><ol><li>如果reduce任务数量大于getPartition返回的最大值，则产生空的输出文件part-r-000xx；</li><li>如果1 &lt; reduce任务数量 &lt; getPartition数量，则抛出IOException异常；</li><li>如果reduce任务数量为1，则只生成一个输出文件part-r-00000。</li></ol><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="排序"></a>排序<a class="hash-link" href="#排序" title="Direct link to heading">#</a></h4><p>MapTask和ReduceTask均会对数据按照key进行排序，这是Hadoop的默认行为，不管程序逻辑上是否需要。</p><p>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。</p><p>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</p><p>排序要求key类型实现WritableComparable接口，即在Writable接口的基础上实现Comparable接口。</p><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="combiner"></a>Combiner<a class="hash-link" href="#combiner" title="Direct link to heading">#</a></h4><p>Combiner作为Reducer的子类，是一种特殊的Reducer。Combiner在每一个MapTask所在的节点运行，而Reducer接收全局所有Mapper的输出结果。Combiner对每个MapTask的输出进行局部汇总，以减少网络传输量。<strong>注意使用Combiner的前提是不影响最终的业务逻辑(比如计算平均数就不行)</strong>，输出K-V和Reducer的输入K-V匹配。</p><p>设置setNumReduceTasks为0，Shuffle阶段不存在，因此Combiner也不生效。业务逻辑相同的可以直接使用Reducer作为Combiner。</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="maptask工作机制"></a>MapTask工作机制<a class="hash-link" href="#maptask工作机制" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="maptask源码分析"></a>MapTask源码分析<a class="hash-link" href="#maptask源码分析" title="Direct link to heading">#</a></h3><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">自定义Mapper类的map方法调用`context.write(key, v)`；</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    WrappedMapper.write(k, v);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    TaskInputOutputContextImpl.write(k, v);</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    MapTask.write(k, v, partitioner.getPartition(key, value, partitions))</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        默认使用HashPartitioner的getPartition方法</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    MapTask.collect()方法将所有K-V写出</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    回到runNewMapper方法中调用output.close() [MapTask.java, Line 805]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        调用collector.flush() [MapTask.java, Line 735]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">            调用sortAndSpill()方法 [MapTask.java, Line 1505]，</span></span><span class="token-line" style="color:#393A34"><span class="token plain">                调用sorter.sort()方法 [MapTask.java, Line 1625]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">            调用mergeParts()方法合并文件 [MapTask.java, Line 1527]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        调用collector.close()，关闭收集器，进入ReduceTask [MapTask.java, Line 739]</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="reducetask源码分析"></a>ReduceTask源码分析<a class="hash-link" href="#reducetask源码分析" title="Direct link to heading">#</a></h3><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">以ReduceTask.run()为入口</span></span><span class="token-line" style="color:#393A34"><span class="token plain">initialize() [ReduceTask.java, Line333]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">shuffleConsumerPlugin.init(shuffleContext) [ReduceTask.java, Line 375]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    new ShuffleSchedulerImpl [Shuffle.java, Line77]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        totalMaps = job.getNumMapTasks() [ShuffleSchedulerImpl.java, Line 120]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    merger = createMergeManager(context) [Shuffle.java, Line 80]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        new MergeManagerImpl [Shuffle.java, Line 85]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">            this.inMemoryMerger = createInMemoryMerger() [MergeManagerImpl.java, Line 232]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">            this.onDiskMerger = new OnDiskMerger(this) [MergeManagerImpl.java, Line 235]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">rIter = shuffleConsumerPlugin.run() [ReduceTask.java, Line 375]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    eventFetcher.start()  [开始抓取数据，Shuffle.java, Line 107]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    eventFetcher.shutDown() [抓取完毕，Shuffle.java, Line 141]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    copyPhase.complete() [copy完成Shuffle.java, Line 151]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    taskStatus.setPhase(TaskStatus.Phase.SORT) [开始排序阶段，Shuffle.java, Line 152]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">sortPhase.complete() [排序阶段完成，进入reduce，ReduceTask.java, Line 382]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">runNewReducer() [进入reducer，ReduceTask.java, Line 390]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    reducer.run() [ReduceTask.java, Line 628]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        setup(); [Reduce.java, Line 168]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        reduce(); [Reduce.java, Line 171]</span></span><span class="token-line" style="color:#393A34"><span class="token plain">        cleanup(); [[Reduce.java, Line 179]]</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="join"></a>Join<a class="hash-link" href="#join" title="Direct link to heading">#</a></h2><p>MapReduce程序的Join操作分为Reduce Join和Map Join两种：</p><ul><li><strong>Reduce Join</strong>：map方法对记录打标签，以连接字段为key，其余字段加标签为value；reduce方法通过标签区分不同来源，然后合并。缺点是所有合并操作在Reduce阶段完成，Map阶段压力小但Reduce阶段压力大，容易产生<strong>数据倾斜</strong>；</li><li><strong>Map Join</strong>：在Map阶段缓存多张表，提前处理业务逻辑，<strong>适合小表关联大表</strong>。
前者通过打标签来区分不同来源的记录，然后用连接字段为key，其余部分加标签为value进行reduce输出。具体操作如下：</li></ul><ol><li>Driver类添加缓存文件：<code>job.addCacheFile(new URI)</code>；</li><li>设置ReduceTask数量为0：<code>job.setNumberReduceTasks(0)</code>；</li><li>Mapper类的setup方法读取缓存文件并保存为集合；</li><li>map方法根据集合和数据完成关联合并。</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="etl"></a>ETL<a class="hash-link" href="#etl" title="Direct link to heading">#</a></h2><p>ETL(Extract-Transform-Load)指数据从源端经过抽取、转换和加载到目标端的过程，在运行核心业务MapReduce程序之前都需要对数据进行清洗，<strong>ETL往往只需要运行Mapper而不需要Reducer</strong>。</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="压缩"></a>压缩<a class="hash-link" href="#压缩" title="Direct link to heading">#</a></h2><p>Hadoop数据压缩的优点是减少IO次数、减少存储空间，但缺点是增加CPU开销。因此使用压缩的原则是CPU密集型应用少用压缩，而IO密集型应用多用压缩。</p><table><thead><tr><th>压缩格式</th><th>Hadoop是否自带</th><th>算法</th><th>扩展名</th><th>是否可切片</th><th>是否需要修改程序</th></tr></thead><tbody><tr><td>DEFALTE</td><td>是</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>否</td></tr><tr><td>GZIP</td><td>是</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>否</td></tr><tr><td>bzip2</td><td>是</td><td>bzip2</td><td>.bz2</td><td><strong>是</strong></td><td>否</td></tr><tr><td>LZO</td><td><strong>否</strong></td><td>LZO</td><td>.lzo</td><td><strong>是</strong></td><td>需要建索引，指定输入格式</td></tr><tr><td>snappy</td><td>是</td><td>Snappy</td><td>.snappy</td><td>否</td><td>否</td></tr></tbody></table><p>当压缩应用于InputFormat，无须显示指定使用的编码方法，Hadoop会自动检查文件扩展名选择合适的解编码方式对文件进行压缩和解压。在开发中若数据量小于块大小，重点考虑压缩和解压比较快的LZO/Snappy，若数据量非常大，重点考虑支持切片的Bzip2和LZO。当压缩应用于Shuffle阶段，重点考虑压缩和解压快的LZO/Snappy。当压缩应用于OutputFormat，如果数据永久保存则考虑压缩率高的bzip2和Gzip，如果作为下一个MapReduce程序的输入，需要考虑是否支持切片。</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="压缩配置"></a>压缩配置<a class="hash-link" href="#压缩配置" title="Direct link to heading">#</a></h3><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs(core-site.xml)</td><td>无，输入<code>hadoop checknative</code>查看</td><td>输入压缩</td><td></td></tr><tr><td>mapreduce.map.output.compress(mapred-site.xml)</td><td>false</td><td>mapper输出</td><td>设置为true开启压缩</td></tr><tr><td>mapreduce.map.output.compress.codec(mapred-site.xml)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>设置LZO或Snappy</td></tr><tr><td>mapreduce.output.fileoutputformat.compress(mapred-site.xml)</td><td>false</td><td>reducer输出</td><td>设置为true开启压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec(mapred-site.xml)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用gzip或bzip2</td></tr></tbody></table></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_3kfx"><div class="col"><b>Tags:</b><ul class="tags_2ga9 padding--none margin-left--sm"><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/hadoop">Hadoop</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/big-data">BigData</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/kayhaw/kayhaw.github.io/edit/master/website/blog/blog/2022-01-31-Learning-Hadoop3.x-03.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_2_ui" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/2022/02/07/Learning-Hadoop3.x-04"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« Hadoop 3.x学习笔记(4)</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/2022/01/30/Learning-Hadoop3.x-02"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Hadoop 3.x学习笔记(2) »</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#mapreduce简介" class="table-of-contents__link">MapReduce简介</a><ul><li><a href="#mapper类" class="table-of-contents__link">Mapper类</a></li><li><a href="#reducer类" class="table-of-contents__link">Reducer类</a></li><li><a href="#driver类" class="table-of-contents__link">Driver类</a></li></ul></li><li><a href="#序列化" class="table-of-contents__link">序列化</a></li><li><a href="#mapreduce框架" class="table-of-contents__link">MapReduce框架</a><ul><li><a href="#job提交流程" class="table-of-contents__link">Job提交流程</a></li><li><a href="#fileinputformat切片" class="table-of-contents__link">FileInputFormat切片</a></li><li><a href="#textinputformat和combinefileinputformat" class="table-of-contents__link">TextInputFormat和CombineFileInputFormat</a></li><li><a href="#shuffle" class="table-of-contents__link">Shuffle</a></li></ul></li><li><a href="#maptask工作机制" class="table-of-contents__link">MapTask工作机制</a><ul><li><a href="#maptask源码分析" class="table-of-contents__link">MapTask源码分析</a></li><li><a href="#reducetask源码分析" class="table-of-contents__link">ReduceTask源码分析</a></li></ul></li><li><a href="#join" class="table-of-contents__link">Join</a></li><li><a href="#etl" class="table-of-contents__link">ETL</a></li><li><a href="#压缩" class="table-of-contents__link">压缩</a><ul><li><a href="#压缩配置" class="table-of-contents__link">压缩配置</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">读书笔记</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://www.zhihu.com/people/xiao-ke-dou-7" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>知乎<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/kayhaw" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://gitee.com/kayhaw" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Gitee<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 何轲(Kay Haw). Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.ed6f7924.js"></script>
<script src="/assets/js/main.87f14a62.js"></script>
</body>
</html>